
Product Requirements Document (PRD)
===================================

<context>
# Overview
This project delivers an **Automated DistilBERT Benchmarking Suite** that measures inference latency, memory footprint, and energy consumption for sequenceâ€‘classification on CPU and GPU inside an HPC cluster.
It solves the pain of adâ€‘hoc, nonâ€‘reproducible model benchmarks by giving ML researchers and infrastructure teams a turnâ€‘key toolchain that produces auditable metrics, plots, and a publicationâ€‘ready report.

# Core Features
1. **Dataset Ingestion & Tokenisation**
   - *What*: Oneâ€‘command download of GLUE/SSTâ€‘2 validation data plus batched tokenisation.
   - *Why*: Ensures reproducible, licenceâ€‘compliant input for every run.
   - *How*: Python scripts using ðŸ¤—Â `datasets` and `DistilBertTokenizerFast`; hash check stored in manifest.

2. **Deviceâ€‘Agnostic Model Loader**
   - *What*: Loads `DistilBertForSequenceClassification` on CPU or any visible CUDA device.
   - *Why*: Allows headâ€‘toâ€‘head benchmarking under identical software versions.
   - *How*: Factory function `load_model(device:str)` with Torch compileâ€‘time flags.

3. **Metric Collectors**
   - *What*: Wallâ€‘clock latency, throughput, peak memory (CPU/GPU), and energy per sample.
   - *Why*: Provides multidimensional view of costÂ vs. performance.
   - *How*: `time.perf_counter()`, `torch.cuda.Event`, `psutil`, `pynvml`, `pyRAPL`.

4. **Batchâ€‘Sweep Runner**
   - *What*: Iterates over configurable batch sizes; dumps one JSON line per run.
   - *Why*: Surfaces scaling behaviour and GPU/CPU crossover points.
   - *How*: Orchestrator `runner.py` driven by YAML config; schemas enforced with `pydantic`.

5. **HPC / SLURM Integration**
   - *What*: Readyâ€‘made `bench_distilbert.slurm` template.
   - *Why*: Enables reproducible cluster executions and queue automation.
   - *How*: Parameterised SBATCH flags, module loads, and rsync of result artefacts.

6. **Analytics & Visualisation**
   - *What*: Scripts generating latency/throughput/memory/energy plots plus Jinjaâ€‘rendered insights markdown.
   - *Why*: Converts raw logs into decisionâ€‘making artefacts for hardware selection.
   - *How*: Pandas + matplotlib; templated summary.

7. **Reporting Pipeline**
   - *What*: Pandocâ€‘powered PDF and PowerPoint slide deck.
   - *Why*: Academicâ€‘ready deliverables without manual copyâ€‘paste.
   - *How*: `make report` target stitches markdown, plots, and bib references.

# User Experience
- **Personas**
  * ML Graduate Student: needs quick, defensible metrics for a paper or coursework.
  * HPC Engineer: validates GPU utilisation and plans capacity.
- **Key User Flows**
  1. Clone repo â†’ `conda env create -f environment.yml` â†’ `make smoke` to validate local setup.
  2. Edit `config.yaml` for desired batch sizes â†’ `make benchmark` (local) or `sbatch bench_distilbert.slurm`.
  3. Retrieve `results-*.json` â†’ `make analyse` â†’ open generated `figures/*.png` and `insights.md`.
  4. `make release` builds `report.pdf` + `slides.pptx`.
- **UI/UX Considerations**
  * CLI with clear help (`--help`) and coloured logging.
  * Result artefacts saved under timestamped directory to avoid collisions.
</context>

<PRD>
# Technical Architecture
- **System Components**
  * `data/` â€“ download & tokenise
  * `src/model.py` â€“ model loader
  * `src/metrics/` â€“ latency, memory, energy collectors
  * `src/runner.py` â€“ benchmark orchestrator
  * `cluster/bench_distilbert.slurm` â€“ SLURM wrapper
  * `analysis/` â€“ Jupyterâ€‘less analysis scripts
- **Data Models**
  * JSON line per run: `{"device": "...", "batch_size": 8, "latency_ms": ..., "energy_J": ...}`
  * Validation schemas via `pydantic.BaseModel`.
- **APIs & Integrations**
  * Hugging Face Hub for model & tokenizer downloads.
  * NVML API for GPU power, memory.
  * pyRAPL for Intel/AMD RAPL counters.
- **Infrastructure Requirements**
  * Local: PythonÂ 3.10+, Conda or Mamba, â‰¥â€¯8â€¯GB RAM.
  * Cluster: SLURM 21+, CUDA 11.8+, NVML read access, outbound internet or preâ€‘cached model.

# Development Roadmap
**PhaseÂ A â€“ MVP**
  * Dataset ingest & hash check
  * CPU benchmarking at batch sizes {1,Â 8,Â 32}
  * JSON result dump, no plots
  * Smoke tests in CI

**PhaseÂ B â€“ GPU Support & Metrics**
  * CUDA device loader
  * GPU latency/memory/energy collectors
  * Batch sweep CLI, structured logging
  * Integration tests

**PhaseÂ C â€“ HPC & Visuals**
  * SLURM script template
  * Automated rsync of artefacts
  * Pandas aggregation + four matplotlib plots
  * Insights markdown

**PhaseÂ D â€“ Reporting & Polish**
  * PDF report via Pandoc
  * 12â€‘slide PowerPoint deck
  * Preâ€‘commit hooks, 90â€¯% test coverage
  * GitHub Release assets + SHA256 manifest

# Logical Dependency Chain
1. Foundation: data ingest âž” tokenisation âž” CPU model load âž” latency metric.
2. Visible MVP: CLI smoke run outputs first JSON & passes unit tests.
3. Extend: add GPU path & advanced metrics (memory, energy).
4. HPC wrapper once local path stable.
5. Analysis & plotting dependent on JSON schema frozen.
6. Reporting after stable analytics.

# Risks and Mitigations
| Risk | Mitigation |
|------|------------|
| NVML or RAPL not accessible on cluster | Provide fallback: average power Ã— duration; document deviation. |
| HuggingÂ Face model download fails (firewall) | Allow offline cache directory; include model artefact checksum. |
| Scope creep in visualisation | Freeze JSON schema early and limit plot count to four. |
| Data drift in SSTâ€‘2 | Pin GLUE dataset version and hashâ€‘check nightly. |

# Appendix
- GLUE SSTâ€‘2 licence (MIT) and citation string.
- Cluster spec sheet (AMD EPYCÂ Milan 7643, NVIDIAÂ A100Â 80â€¯GB, SLURMÂ 21.08).
- Example result JSON and plot thumbnails.
</PRD>
